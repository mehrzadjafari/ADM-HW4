{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Hashing\n",
    "\n",
    "<br>\n",
    "\n",
    "* Download the datasets [here](https://drive.google.com/file/d/19SD2db0dH2A0QLJOmBHnkbqOX6SbERcY/view?usp=sharing). You will find a `hash.txt` file.\n",
    "<br>\n",
    "\n",
    "1. Implement your hash functions from scratch, no ready-made hash functions are allowed. Read the class material and search the internet if you need to. As a reference, it may be useful to look at the description of hash functions in the [book](http://www.mmds.org/) or [here](http://aris.me/contents/teaching/data-mining-ds-2020/resources/DPV-universal-hashing.pdf).\n",
    "<br>\n",
    "\n",
    "2. Use your hash function, implement a HyperLogLog structure.\n",
    "<br>\n",
    "\n",
    "3. Read the dataset sequentially and add it to your HyperLogLog.\n",
    "<br>\n",
    "\n",
    "4. At the end you have to provide:\n",
    "    * The cardinality of the dataset.\n",
    "    * The error of your filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments:**\n",
    "\n",
    "\n",
    "Importing the libraries to work with arrays and math operation as the square root.\n",
    "\n",
    "Importing `hashing_functions` module that contains our functions for this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import hashing_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have created a custom function called `zaa` that is responsible for choose a size of the vector, and as output we have the hashing key and the number of zeros. \n",
    "<br>\n",
    "\n",
    "\n",
    "The function is used inside the function `part_hill` that receives an array which contains the hash keys and the counted number of zeros. The proocedure is to create the buckets using the hashkeys and save the respective values without collision treatment. As an output this fuction give the sum of the number of zeros of the array and also the number of buckets.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "The function `part_hill` has been used inside the function `split_data` which is responsible to split the huge amount of data we need to treat, apply the algorithm in part of it and then sum all the partial results from the function `part_hill` and give as an output the result of the probably **cardinality** of the data and the respective **error**.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "For more info please read the docstrings of `hashing_functions.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cardinality: 173554702\n",
      "Error: 1.2327952679426826\n"
     ]
    }
   ],
   "source": [
    "f = open(\"hash.txt\")\n",
    "f = f.readlines()\n",
    "[cardinality,error] = hashing_functions.split_data(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clustering\n",
    "<br>\n",
    "\n",
    "We play with a dataset gathering reviews ($\\sim$\t560k) of [fine foods from Amazon](https://www.kaggle.com/snap/amazon-fine-food-reviews). The reviews include much information. We focus on the reviews' plain text and try to cluster the products ($\\sim$\t74).\n",
    "\n",
    "\n",
    "To solve this task, you must:\n",
    "\n",
    "\n",
    "1. Implement the k-means clustering algorithm (not ++: random initialization). We ask you to write the algorithm from scratch following what you learned in class.\n",
    "<br>\n",
    "\n",
    "2. Run the algorithm on the food data. Then, use the already implemented version of k-means++, are there any differences in results?\n",
    "<br>\n",
    "\n",
    "3. Analyse the obtained clusters:\n",
    "\n",
    "    * Identify the kind of products in the cluster (e.g., chips, tea, coffee) using a visualization called word cloud.\n",
    "    * Provide the number of product in each cluster\n",
    "    * Compute the reviews' score distribution in each cluster. Once you get them, test if their mean differences are statistically significant!\n",
    "    * Get the number of unique users writing reviews in each cluster\n",
    "    \n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "   \n",
    "Before running the algorithm, you should consider the following:\n",
    "<br>\n",
    "\n",
    "* How do you represent the data? (e.g., do I use a binary representation or TF-IDF?)\n",
    "<br>\n",
    "\n",
    "* How do you pre-process data? Since you aim to characterize products by their review, do you want to consider words that appear in too many or too few documents?\n",
    "<br>\n",
    "\n",
    "* After organizing your data, you will realize that tens of thousands of words compose your vocabulary. In this case, we suggest you to use the SVD method to reduce the dimensionality of the dataset. This operation is typically used to denoise the data and implies the loss of some information. For this reason, you first reduce your dataset to a few hundred components (e.g., 100). Then, you use the following np.cumsum(explained_variance_ratio_) to verify the total amount of variance you retain with an increasing number of components. You can pick a number of components that retain> 60% of the variance. Since we know this point can raise many questions, opening a thread on Slack is recommended and welcomed.\n",
    "<br>\n",
    "\n",
    "* The choice of the number of clusters should not be random.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**IMPORTANT**: We are aware that you may consult the internet for information about implementing the requested algorithms. However, the final code must be yours! So please, do not search and copy-paste the code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Algorithmic question\n",
    "<br>\n",
    "\n",
    "You are given an array with A with n integer numbers.\n",
    "<br>\n",
    "\n",
    "* Let s = min{ A[1], ..., A[n] } and b = max { A[1], ..., A[n] }.\n",
    "<br>\n",
    "\n",
    "* Let r = b - s\n",
    "<br>\n",
    "\n",
    "Prove that we can sort A in time O(n + r)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
